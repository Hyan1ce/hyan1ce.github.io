---
title: DL learning notes -- Part 1
published: 2025-05-13
description: 'Pytorch learning notes, references: LeeDL-Tutorial'
image: ''
tags: [PyTorch, DL, ML]
category: 'Notes'
draft: false 
lang: ''


---

> 参考：李宏毅 深度学习教程：[LeeDL-Tutorial](https://github.com/datawhalechina/leedl-tutorial)

# Chapter 1

## 机器学习流程

1. 写出一个带有未知参数的函数: $y=b+w_1x$

2. **定义损失**用来衡量函数好坏

   1. 平均绝对误差

   2. 均方误差

      ![image-20250909154537747](assets/image-20250909154537747.png)

3. **解最优化问题**，寻找最优的 $b, w_1$；**梯度下降**是常用的方法。

**梯度下降**：设定**学习率**，让机器自己找到最优参数的过程；可能只能找到局部最小值；可通过调整学习率来改善（但是调整了也未必能找到全局最小值）

**流程图**：

![image-20250909161844749](assets/image-20250909161844749.png)

## 激活函数

**Sigmoid**：倒S型，可以通过调整 $b,w,c$ 得到不同的形状：
$$
y=c \frac {1+e^{-(b+wx_1)}}{1}
$$
![image-20250909160237187](assets/image-20250909160237187.png)

**ReLU**: 开始是0，某位置之后变为非0的一次函数：$y=c \times \max(0,b+wx_1)$

## 名词定义总结

**超参数**：需要自己设定的参数（如学习率）

**神经元**：ReLU，Sigmoid都算

**隐藏层**：神经网络中每一排称为一个隐藏层

**过拟合**：训练数据和测试数据上的结果不一致

> 如在训练数据上，3层比4层差；但是在没看过的数据上，4层比较差，3层比较好

# Chapter 2

**解决过拟合**：1. 增加训练数据；2. 给模型增加限制，如规定其为二次函数

**交叉验证**：把训练的数据分成两半，一部分称为训练集（training set），一部分是验证集（validation set），如k折交叉验证、留一法验证等

# Chapter 3

## 临界点/鞍点

**临界点**：梯度为0的点统称为临界点。<u>梯度（损失关于参数的微分）</u>为0时，损失无法继续下降，此时既有可能收敛在了**局部极小值点**，也可能收敛在**鞍点**。

**鞍点**：梯度为0，但并不是**局部极小/大值点**的点：

![image-20250909163708309](assets/image-20250909163708309.png)

**判断个临界点到底是局部极小值还是鞍点**：利用泰勒展开式，推导过程跳过

> 其他内容用不上，暂时跳过

# Chapter 4

## 将图像输入模型

**将图像输入模型**：分为3个通道（RGB），分别表明R, G, B在某一个通道下的颜色强度；如 $100 \times 100$的图像的输入向量长度为 $100 \times 100 \times 3$

**分类模型的输出**：独热向量，即分类结果的位是1，其他位是0；向量的长度代表了分类结果的数量：

![image-20250910134942129](assets/image-20250910134942129.png)

## 感受野

> [!TIP]
>
> **观察 1**：检测模式不需要整张图像

**感受野**：根据观察1，卷积神经网络会设定一个区域，即**感受野（receptive field）**，每个神经元都只关心自己的感受野里面发生的事情，<u>感受野是由我们自己决定的</u>。

- 感受野可以相连
- 感受野可以只考虑部分通道（但一般来说都考虑全部通道，因为不会觉得有些模式只出现在某一个通道里面）
- **核大小**：高、宽，如 $3 \times 3$，深度就等于通道数，所以一般不额外描述
- 感受野超出图像范围时，需要进行**填充**，可以补0/采用其他填充方法
- 感受野盖满整个图像，保证所有位置都有神经元去监督

**步幅（stride）**：感受野上下左右移动的像素数，是一个**超参数**；$3 \times 3$的感受野情况下步幅一般为2，因为可以保证感受野重叠。

> **Q：为何要保证感受野重叠？**
>
> A：假设感受野完全没有重叠，如果有一个模式正好出现在两个感受野的交界上面， 就没有任何神经元去检测它，这个模式可能会丢失，所以希望感受野彼此之间有高度 的重叠。如令步幅 = 2，感受野就会重叠。



> [!TIP]
>
> **观察 2**：同样的模式可能会出现在图像的不同区域

**简化方法**：对不同的感受野采用相同的参数，也就是**参数共享（parameter sharing**），所以每个感受野都只有一组参数，这些参数称为**滤波器（filter）**

> 因为输入不一样，所以就算是两个神经元共用参数，它们的输出也不会是一样的

![image-20250910142926085](assets/image-20250910142926085.png)
